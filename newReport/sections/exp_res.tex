\section{Evaluation}
\label{sec:exps}
As mentioned in the previous section, the corpus is filtered based on the weather tags that are produced by the current day's weather. Therefore, checking for word relevance in the generated sentences to the input is not required. Also, any grammatical evaluation is skipped, because it is exclusively dependent on the initial CFG, which, if written carefully, would not allow for any grammatical mistakes in the generated output.

For the evaluation, two types of tests were considered, automated similarity measures, namely, String-Edit distance and BLEU \cite{Papineni2001}, and a human perception test through an online survey. The methods' generalization performance is calculated through cross-validation using the automated metrics. However, String-edit and BLEU have a limitation that the semantics of the generated sentences are not considered. This gap was filled by a human perception test that was conducted. Details for all the methods can be found in the following paragraphs.

\subsection{String-Edit distance}
The String-edit metric is based on the standard String-edit distance between two sentences, which is the sum of all additions, moves and substitutions needed to transform the first sentence into the other. For example the String-edit distance between ``take your hat today'' and ``wear your hat'' is two. The lower the distance, the more similar the two sentences are.
\subsection{BLEU}
BLEU is an evaluation metric for machine translation and has recently been adopted by the NLG community due to its effectiveness. It is based on a weighted average of different n-grams, as a measure of similarity. Every generated sentence is compared to a set of reference sentences and the score is calculated for every reference sentence. The minimum score is picked from the reference scores to account. First, it counts commonly occurring uni-grams (i.e. words) between both reference and generated sentences. This number is divided by the total number of uni-grams in the generated sentence. The same number is then calculated again for bi-grams, tri-grams, ..., n-gram occurrences. In the end weighted average sum of these counts is calculated. It is worth mentioning, that these counts decay exponentially, therefore, BLEU uses the average logarithm with uniform weights. BLEU has shown correlation with monolingual human judgement using a maximum n-gram of order 4-grams. Therefore, that is, what was used for the experiments that were conducted.

\subsection{Perception Test}
The perception test is one of the human rating and judgement methods of evaluation, where the generated wardrobe tips were evaluated by readers. The participants are presented with groups of sentences that are mix of human written and computer generated sentences and are asked to select the sentences, that they believe are produced by a human, taking into consideration syntax, semantics and relevance to the input weather conditions. So, for different weather conditions, tips are generated using all the generators mentioned in the previous sections. These generated texts along with a sentence from the corpus, are given out to people in the form off a survey. From the responses of the survey, the percentage of all computer generated sentences that had been voted as human-written, was calculated. These numbers represent the effectiveness of the generator that produced the corresponding tips. The different generators are then compared, based on their effectiveness.

\subsection{Experimental Setup}
The built corpus was small; it contained 51 sentences and 1438 words, of which 51 were unique. The corpus was divided into three, almost equal, folds, so that a three-fold cross-validation could be performed. The Treebanks were created using different parsers, as described in Sections \ref{subsec:parsers} \& \ref{subsec:impl}. Then, the PCFG was created using the training fold of the corpus and, finally, for all combinations of the weather tags in the validation fold, tips were produced, using different generators, as described in Sections  \ref{subsec:generators} \& \ref{subsec:impl}. For the tips generated by all the different parsers and generators, string-edit distance and BLEU metric were calculated, with regard to all the sentences in the validation folds. For the randGen and probGen methods, an average value over thirty repetitions is calculated. After the metrics are calculated for every tip, the average value over all the sentences in each fold is computed.


%\begin{table}
%\caption{Corpus Sample}\label{tab:corpus}
%\begin{center}
%\begin{tabular}{|c|c|}
%\hline
%hot/rainy/sunny/windy &  you have to take your raincoat \\ \hline
%cold/dry/sunny/windy & remember the blazer \\ \hline
%calm/cloudy/dry/warm & do not wear many layers \\ \hline
%calm/dry/hot/sunny & wear your sunglasses \\ \hline
%\end{tabular}
%\end{center}
%\end{table}
%\begin{algorithm}[H]
% \For{every method}{
%  \For{every parser}{
%  \For{every validation fold}{
%   build PCFG using two training folds
%   
%   \For{every set of tags in the validation fold}{
%	generate sentence using method, parser and PCFG
%	
%	\For{every sentence in validation fold relevant to set of tags}{
%		calculate string edit distance to generated sentence
%	
%		calculate BLEU to generated sentence
%	}
%	get minimum String-Edit distance over all sentences
%	
%	get maximum BLEU over all sentences
%	
%  \If{method is randGen or probGen}{
%	repeat above 30 times and take average   
%  }
% }
% average String-Edit distance over fold.
% 
% average BLEU over fold
% }
% }
% }
% \caption{Cross-validation using BLEU and String-Edit}
%\end{algorithm}
\subsection{Results}
It has been observed that all parsers generated the exact same parse trees for the CFG. This is due to the simplicity of the CFG. Automated metric evaluation results for every method and every fold are presented in Figure~\ref{fig:Auto_Results}. For BLEU, higher results represent better performance, whereas for string-edit higher results represent higher error. It is noticeable that, the performance of Viterbi is better than Greedy. This is consistent with \cite{Belz}. However, unlike \cite{Belz}, the best algorithm in this evaluation was Viterbi, not n-gram. All the methods were better than randGen, however, probGen and n-gram had only a mild enhancement over randGen.

\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{pictures/bleu.jpg}~
\includegraphics[width=0.5\linewidth]{pictures/se.jpg}
\caption{Model results for BLEU and String Edit}
\label{fig:Auto_Results}
\end{figure}

\subsection{Discussion}
The results that were obtained from the automated metrics are measures of similarity between the generated sentences and some reference sentences. However, these metrics are unable to test if a generated sentence does, in fact, make sense with regard to the weather conditions. For instance, comparing the sentence ``Wear your raincoat'' to the sentence ``Take your umbrella'' results in a large error for both metrics, however, both sentences are correct responses for a rainy weather. Representing this relationship between the sentence and weather condition can be done by making an extensive corpus, that would allow for many versions of the same sentence, so that it is guaranteed that sentences in a given fold, can be generated by sentences in the remaining folds. However, for a small corpus, an enhancement would be to account for the variation in the folds. That is, to compare the methods' performance for each testing fold, to the average similarity of the testing and training folds. This allows for a better, yet still incomplete, insight to the methods' performance. As seen in Table~\ref{tab:foldsErr}, the string-edit error of the best methods is slightly better than the fold to fold error.

What can also be derived from the above, is that some sentences, that had a low score on the automated metrics, had a good score on the human evaluation metrics.

\begin{table}
\caption{Fold to fold error and comparison to Viterbi and Greedy for String-edit}\label{tab:foldsErr}
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|}
\hline
Fold & Fold Error & Viterbi Error & Greedy Error \\ \hline
Fold 1 & 0.528074795575 & 0.597222222222 & 0.647916666667 \\ \hline
Fold 2 & 0.532187049062 & 0.569444444444 & 0.54375 \\ \hline
Fold 3 & 0.53913270757 & 0.575 & 0.539583333333 \\ \hline
\end{tabular}
\end{center}
\end{table}